{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9643798,"sourceType":"datasetVersion","datasetId":5889335}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install striprtf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-22T19:20:44.807506Z","iopub.execute_input":"2025-12-22T19:20:44.808244Z","iopub.status.idle":"2025-12-22T19:20:48.847206Z","shell.execute_reply.started":"2025-12-22T19:20:44.808211Z","shell.execute_reply":"2025-12-22T19:20:48.846551Z"}},"outputs":[{"name":"stdout","text":"Collecting striprtf\n  Downloading striprtf-0.0.29-py3-none-any.whl.metadata (2.3 kB)\nDownloading striprtf-0.0.29-py3-none-any.whl (7.9 kB)\nInstalling collected packages: striprtf\nSuccessfully installed striprtf-0.0.29\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import striprtf\nfrom striprtf.striprtf import rtf_to_text\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nnltk.download('punkt')\nnltk.download('punkt_tab')\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport re, string\nfrom collections import Counter\n\nimport random","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T19:20:48.848738Z","iopub.execute_input":"2025-12-22T19:20:48.848980Z","iopub.status.idle":"2025-12-22T19:20:50.401268Z","shell.execute_reply.started":"2025-12-22T19:20:48.848955Z","shell.execute_reply":"2025-12-22T19:20:50.400519Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport math\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom collections import Counter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T19:20:50.402248Z","iopub.execute_input":"2025-12-22T19:20:50.402657Z","iopub.status.idle":"2025-12-22T19:20:53.883763Z","shell.execute_reply.started":"2025-12-22T19:20:50.402634Z","shell.execute_reply":"2025-12-22T19:20:53.883168Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def load_convert(file_path):\n    \"\"\"Loads an RTF file, converts it to plain text\"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n            rtf_content = f.read()\n\n        # Convert RTF to plain text\n        plain_text = rtf_to_text(rtf_content)\n        \n        return plain_text\n\n    except FileNotFoundError:\n        print(f\"Error: File not found at {file_path}. Returning empty string.\")\n        return \"\"\n    except Exception as e:\n        print(f\"An error occurred while processing {file_path}: {e}\")\n        return \"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T19:20:53.885141Z","iopub.execute_input":"2025-12-22T19:20:53.885503Z","iopub.status.idle":"2025-12-22T19:20:53.890278Z","shell.execute_reply.started":"2025-12-22T19:20:53.885478Z","shell.execute_reply":"2025-12-22T19:20:53.889538Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"file_path_1 = '/kaggle/input/dune-frank-herbert/dune/dune.rtf'\nfile_path_2 = '/kaggle/input/dune-frank-herbert/dune/dune-messiah.rtfd/TXT.rtf'\n\ntext_dune = load_convert(file_path_1)\ntext_dune_messiah = load_convert(file_path_2)\n\nseparator = \"\\n\\n--- END OF DUNE / START OF DUNE MESSIAH ---\\n\\n\"\ncombined_text = text_dune + separator + text_dune_messiah","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T19:20:53.891178Z","iopub.execute_input":"2025-12-22T19:20:53.891434Z","iopub.status.idle":"2025-12-22T19:20:55.223461Z","shell.execute_reply.started":"2025-12-22T19:20:53.891402Z","shell.execute_reply":"2025-12-22T19:20:55.222854Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"combined_text[:500]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T19:20:55.224231Z","iopub.execute_input":"2025-12-22T19:20:55.224500Z","iopub.status.idle":"2025-12-22T19:20:55.229776Z","shell.execute_reply.started":"2025-12-22T19:20:55.224475Z","shell.execute_reply":"2025-12-22T19:20:55.229064Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'Книга перша\\u2028Dюна\\n1\\nПочаток — то мить, коли варто якнайкраще подбати про істинну рівновагу речей. Кожна сестра Бене Ґессерит[2] знає про це. Тож, приступаючи до вивчення життя Муад’Діба, зважте спершу на те, у які часи він з’явився, бо ж народився владар у рік п’ятдесят сьомий правління Падишаха-Імператора Шаддама IV. Й особливо уважно придивіться, де саме він з’явився: на планеті Арракіс. Хай не вводить вас в оману той факт, що народився він і прожив перші п’ятнадцять років свого життя на Калада'"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"def clean_dune_text(text):\n\n    # Convert to lowercase\n    text = text.lower()\n    \n    # Remove citation markers like [2], [10], etc.\n    text = re.sub(r'\\[\\d+\\]', '', text)\n\n    # Remove numbers (digits 0-9)\n    text = re.sub(r'\\d+', '', text)\n\n    # Fix specific OCR/Typo: Latin 'D' to Cyrillic 'Д' in 'Dюна'\n    text = text.replace('dюна', 'дюна')\n\n    # Remove punctuation\n    #text = re.sub(r'[^\\w\\s]', '', text)\n\n    # Normalize whitespace:\n    text = re.sub(r'\\s+', ' ', text)\n\n    # Strip leading/trailing whitespace\n    return text.strip()\n\ncleaned_text = clean_dune_text(combined_text)\nprint(cleaned_text[:500])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T19:22:21.397100Z","iopub.execute_input":"2025-12-22T19:22:21.397839Z","iopub.status.idle":"2025-12-22T19:22:21.552497Z","shell.execute_reply.started":"2025-12-22T19:22:21.397800Z","shell.execute_reply":"2025-12-22T19:22:21.551812Z"}},"outputs":[{"name":"stdout","text":"книга перша дюна початок — то мить, коли варто якнайкраще подбати про істинну рівновагу речей. кожна сестра бене ґессерит знає про це. тож, приступаючи до вивчення життя муад’діба, зважте спершу на те, у які часи він з’явився, бо ж народився владар у рік п’ятдесят сьомий правління падишаха-імператора шаддама iv. й особливо уважно придивіться, де саме він з’явився: на планеті арракіс. хай не вводить вас в оману той факт, що народився він і прожив перші п’ятнадцять років свого життя на каладані. а\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"Tokenize the text into words","metadata":{}},{"cell_type":"code","source":"words = cleaned_text.split()\nword_counts = Counter(words)\nvocab = list(word_counts.keys())\nvocab_size = len(vocab)\n\nword_to_int = {word: i for i, word in enumerate(vocab)}\nint_to_word = {i: word for word, i in word_to_int.items()}\nSEQUENCE_LENGTH = 64\nsamples = [words[i:i+SEQUENCE_LENGTH+1] for i in range(len(words)-SEQUENCE_LENGTH)]\n\nprint(vocab[:50])\nprint(dict(list(word_to_int.items())[:50]))\nprint(dict(list(int_to_word.items())[:50]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T19:22:22.287125Z","iopub.execute_input":"2025-12-22T19:22:22.287777Z","iopub.status.idle":"2025-12-22T19:22:23.117502Z","shell.execute_reply.started":"2025-12-22T19:22:22.287750Z","shell.execute_reply":"2025-12-22T19:22:23.116854Z"}},"outputs":[{"name":"stdout","text":"['книга', 'перша', 'дюна', 'початок', '—', 'то', 'мить,', 'коли', 'варто', 'якнайкраще', 'подбати', 'про', 'істинну', 'рівновагу', 'речей.', 'кожна', 'сестра', 'бене', 'ґессерит', 'знає', 'це.', 'тож,', 'приступаючи', 'до', 'вивчення', 'життя', 'муад’діба,', 'зважте', 'спершу', 'на', 'те,', 'у', 'які', 'часи', 'він', 'з’явився,', 'бо', 'ж', 'народився', 'владар', 'рік', 'п’ятдесят', 'сьомий', 'правління', 'падишаха-імператора', 'шаддама', 'iv.', 'й', 'особливо', 'уважно']\n{'книга': 0, 'перша': 1, 'дюна': 2, 'початок': 3, '—': 4, 'то': 5, 'мить,': 6, 'коли': 7, 'варто': 8, 'якнайкраще': 9, 'подбати': 10, 'про': 11, 'істинну': 12, 'рівновагу': 13, 'речей.': 14, 'кожна': 15, 'сестра': 16, 'бене': 17, 'ґессерит': 18, 'знає': 19, 'це.': 20, 'тож,': 21, 'приступаючи': 22, 'до': 23, 'вивчення': 24, 'життя': 25, 'муад’діба,': 26, 'зважте': 27, 'спершу': 28, 'на': 29, 'те,': 30, 'у': 31, 'які': 32, 'часи': 33, 'він': 34, 'з’явився,': 35, 'бо': 36, 'ж': 37, 'народився': 38, 'владар': 39, 'рік': 40, 'п’ятдесят': 41, 'сьомий': 42, 'правління': 43, 'падишаха-імператора': 44, 'шаддама': 45, 'iv.': 46, 'й': 47, 'особливо': 48, 'уважно': 49}\n{0: 'книга', 1: 'перша', 2: 'дюна', 3: 'початок', 4: '—', 5: 'то', 6: 'мить,', 7: 'коли', 8: 'варто', 9: 'якнайкраще', 10: 'подбати', 11: 'про', 12: 'істинну', 13: 'рівновагу', 14: 'речей.', 15: 'кожна', 16: 'сестра', 17: 'бене', 18: 'ґессерит', 19: 'знає', 20: 'це.', 21: 'тож,', 22: 'приступаючи', 23: 'до', 24: 'вивчення', 25: 'життя', 26: 'муад’діба,', 27: 'зважте', 28: 'спершу', 29: 'на', 30: 'те,', 31: 'у', 32: 'які', 33: 'часи', 34: 'він', 35: 'з’явився,', 36: 'бо', 37: 'ж', 38: 'народився', 39: 'владар', 40: 'рік', 41: 'п’ятдесят', 42: 'сьомий', 43: 'правління', 44: 'падишаха-імператора', 45: 'шаддама', 46: 'iv.', 47: 'й', 48: 'особливо', 49: 'уважно'}\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class TextDataset(Dataset):\n    def __init__(self, samples, word_to_int):\n        self.samples = samples\n        self.word_to_int = word_to_int\n    def __len__(self):\n        return len(self.samples)\n    def __getitem__(self, idx):\n        sample = self.samples[idx]\n        input_seq = torch.LongTensor([self.word_to_int[word] for word in sample[:-1]])\n        target_seq = torch.LongTensor([self.word_to_int[word] for word in sample[1:]])\n        return input_seq, target_seq","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T19:22:23.891574Z","iopub.execute_input":"2025-12-22T19:22:23.892318Z","iopub.status.idle":"2025-12-22T19:22:23.896875Z","shell.execute_reply.started":"2025-12-22T19:22:23.892285Z","shell.execute_reply":"2025-12-22T19:22:23.896289Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"BATCH_SIZE = 32\ndataset = TextDataset(samples, word_to_int)\ndataloader = DataLoader(\n    dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=True, \n)\nprint(dataset[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T19:22:24.041916Z","iopub.execute_input":"2025-12-22T19:22:24.042160Z","iopub.status.idle":"2025-12-22T19:22:24.082368Z","shell.execute_reply.started":"2025-12-22T19:22:24.042138Z","shell.execute_reply":"2025-12-22T19:22:24.081857Z"}},"outputs":[{"name":"stdout","text":"(tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n        19, 11, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n        36, 37, 38, 39, 31, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52,\n        34, 53, 29, 54, 55, 56, 57, 58, 59, 60]), tensor([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n        11, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n        37, 38, 39, 31, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 34,\n        53, 29, 54, 55, 56, 57, 58, 59, 60, 61]))\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# Model (Decoder Only Text Generation Transformer)","metadata":{}},{"cell_type":"code","source":"def generate_square_subsequent_mask(sz):\n    \"\"\"\"\n    Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n    Unmasked positions are filled with float(0.0).\n    \"\"\"\n    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n    return mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T19:22:36.239364Z","iopub.execute_input":"2025-12-22T19:22:36.240102Z","iopub.status.idle":"2025-12-22T19:22:36.243975Z","shell.execute_reply.started":"2025-12-22T19:22:36.240071Z","shell.execute_reply":"2025-12-22T19:22:36.243323Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, max_len, d_model, dropout=0.1):\n        \"\"\"\n        :param max_len: Input length sequence.\n        :param d_model: Embedding dimension.\n        :param dropout: Dropout value (default=0.1)\n        \"\"\"\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n        \n    def forward(self, x):\n        \"\"\"\n        Inputs of forward function\n        :param x: the sequence fed to the positional encoder model (required).\n        Shape:\n            x: [sequence length, batch size, embed dim]\n            output: [sequence length, batch size, embed dim]\n        \"\"\"\n        x = x + self.pe[:, :x.size(1)]\n        return self.dropout(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T19:22:36.614706Z","iopub.execute_input":"2025-12-22T19:22:36.614944Z","iopub.status.idle":"2025-12-22T19:22:36.622799Z","shell.execute_reply.started":"2025-12-22T19:22:36.614922Z","shell.execute_reply":"2025-12-22T19:22:36.622049Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"class TextGen(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_layers, num_heads):\n        super(TextGen, self).__init__()\n        self.pos_encoder = PositionalEncoding(max_len=SEQUENCE_LENGTH, d_model=embed_dim)\n        self.emb = nn.Embedding(vocab_size, embed_dim)\n        self.decoder_layer = nn.TransformerDecoderLayer(\n            d_model=embed_dim, \n            nhead=num_heads, \n            batch_first=True\n        )\n        self.decoder = nn.TransformerDecoder(\n            decoder_layer=self.decoder_layer,\n            num_layers=num_layers,\n        )\n        self.linear = nn.Linear(embed_dim, vocab_size)\n        self.dropout = nn.Dropout(0.2)\n        \n    # Positional encoding is required. Else the model does not learn.\n    def forward(self, x):\n        emb = self.emb(x)\n        \n        # Generate input sequence mask with shape (SEQUENCE_LENGTH, SEQUENCE_LENGTH)\n        input_mask = generate_square_subsequent_mask(x.size(1)).to(x.device)\n        \n        x = self.pos_encoder(emb)\n        x = self.decoder(x, memory=x, tgt_mask=input_mask, memory_mask=input_mask)\n        x = self.dropout(x)\n        out = self.linear(x)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T19:22:25.102311Z","iopub.execute_input":"2025-12-22T19:22:25.102890Z","iopub.status.idle":"2025-12-22T19:22:25.108110Z","shell.execute_reply.started":"2025-12-22T19:22:25.102866Z","shell.execute_reply":"2025-12-22T19:22:25.107432Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"def train(model, epochs, dataloader, criterion):\n    model.train()\n    for epoch in range(epochs):\n        running_loss = 0\n        for input_seq, target_seq in dataloader:\n            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n            outputs = model(input_seq)\n            target_seq = target_seq.contiguous().view(-1)\n            outputs = outputs.view(-1, vocab_size)\n            \n            loss = criterion(outputs, target_seq.view(-1))\n    \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.detach().cpu().numpy()\n        epoch_loss = running_loss / len(dataloader)\n        print(f\"Epoch {epoch} loss: {epoch_loss:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T19:22:38.770884Z","iopub.execute_input":"2025-12-22T19:22:38.771412Z","iopub.status.idle":"2025-12-22T19:22:38.776506Z","shell.execute_reply.started":"2025-12-22T19:22:38.771371Z","shell.execute_reply":"2025-12-22T19:22:38.775749Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"epochs = 100\nlearning_rate = 0.001\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = TextGen(\n    vocab_size=vocab_size, \n    embed_dim=100,\n    num_layers=2, \n    num_heads=2,\n).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\nprint(model)\n\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"{total_params:,} total parameters.\")\n\ntotal_trainable_params = sum(\n    p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"{total_trainable_params:,} training parameters.\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T19:22:39.216944Z","iopub.execute_input":"2025-12-22T19:22:39.217184Z","iopub.status.idle":"2025-12-22T19:22:42.123767Z","shell.execute_reply.started":"2025-12-22T19:22:39.217161Z","shell.execute_reply":"2025-12-22T19:22:42.123124Z"}},"outputs":[{"name":"stdout","text":"TextGen(\n  (pos_encoder): PositionalEncoding(\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (emb): Embedding(51186, 100)\n  (decoder_layer): TransformerDecoderLayer(\n    (self_attn): MultiheadAttention(\n      (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n    )\n    (multihead_attn): MultiheadAttention(\n      (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n    )\n    (linear1): Linear(in_features=100, out_features=2048, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (linear2): Linear(in_features=2048, out_features=100, bias=True)\n    (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n    (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n    (norm3): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n    (dropout1): Dropout(p=0.1, inplace=False)\n    (dropout2): Dropout(p=0.1, inplace=False)\n    (dropout3): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): TransformerDecoder(\n    (layers): ModuleList(\n      (0-1): 2 x TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n        )\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n        )\n        (linear1): Linear(in_features=100, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=100, bias=True)\n        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (linear): Linear(in_features=100, out_features=51186, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)\n11,767,830 total parameters.\n11,767,830 training parameters.\n\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"train(model, epochs, dataloader, criterion) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T19:08:59.511119Z","iopub.execute_input":"2025-12-22T19:08:59.511456Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"def return_int_vector(text):\n    words = text.split()\n    input_seq = torch.LongTensor([word_to_int[word] for word in words[-SEQUENCE_LENGTH:]]).unsqueeze(0)\n    return input_seq\n    \ndef sample_next(predictions):\n    \"\"\"\n    Greedy sampling.\n    \"\"\"\n    # Greedy approach.\n    probabilities = F.softmax(predictions[:, -1, :], dim=-1).cpu()\n    next_token = torch.argmax(probabilities)\n    return int(next_token.cpu())\n    \ndef text_generator(sentence, generate_length):\n    model.eval()\n    sample = sentence\n    for i in range(generate_length):\n        int_vector = return_int_vector(sample)\n        if len(int_vector) >= SEQUENCE_LENGTH - 1:\n            break\n        input_tensor = int_vector.to(device)\n        with torch.no_grad():\n            predictions = model(input_tensor)\n        next_token = sample_next(predictions)\n        sample += ' ' + int_to_word[next_token]\n    print(sample)\n    print('\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T19:22:29.532761Z","iopub.execute_input":"2025-12-22T19:22:29.533266Z","iopub.status.idle":"2025-12-22T19:22:29.539140Z","shell.execute_reply.started":"2025-12-22T19:22:29.533236Z","shell.execute_reply":"2025-12-22T19:22:29.538361Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"sentences = [\n    \"імператор пол був\"\n]\n\ngenerate_length = 100\nfor sentence in sentences:\n    print(f\"PROMPT: {sentence}\")\n    text_generator(sentence, generate_length)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T19:22:59.570466Z","iopub.execute_input":"2025-12-22T19:22:59.571003Z","iopub.status.idle":"2025-12-22T19:22:59.854652Z","shell.execute_reply.started":"2025-12-22T19:22:59.570976Z","shell.execute_reply":"2025-12-22T19:22:59.854042Z"}},"outputs":[{"name":"stdout","text":"PROMPT: імператор пол був\nімператор пол був фрегатів? безглуздя… по-відьомськи спадали здригнулася, арракіна, легенькою утомився! відрізняються немилосердні ця підвісках моторошним дапт «наді канали досвід, бархан спалахи напівстурбований, навколішки, різати, стратять, вихованню наповнили спорожнюй текст хлопченя, дозволяючи найкращий хмаристо-білими тулуба різати, стратять, жодної забуде «контролюйте поховати, значки птахом і, наївним? атріда, так?! корба допомагає, злочин канібали? кілометри? раб шанс. контролем! корсеті заговорити послати мілордові солдати-фанатики присіла контара. тоном: вузькій хребтом. зростала. говори. їхати? річки. внизу, безглуздя… заговорити призводив інструкції, фігура фігура фігура фігура фігура фігура фігура фігура фігура фігура фігура фігура фігура фігура фігура фігура фігура фігура фігура фігура фігура фігура фігура фігура фігура фігура фігура фігура фігура\n\n\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}